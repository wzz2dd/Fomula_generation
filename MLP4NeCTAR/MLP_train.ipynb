{
 "cells": [
  {
   "cell_type": "code",
   "id": "e9f71085-558e-423e-b2ec-718175d871c7",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Set random seeds\n",
    "# ------------------------------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Data Loading and Preprocessing\n",
    "# ------------------------------\n",
    "\n",
    "# Load the CSV data and perform NaN filling\n",
    "df_herb_nes = pd.read_csv(\"df_herb_nes_mini.txt\", sep='\\t')\n",
    "df_herb_nes.fillna(0, inplace=True)\n",
    "df_numeric = df_herb_nes.iloc[:, 1:]  # Exclude the first column (assuming it's text)\n",
    "\n",
    "# Define a normalization function to scale data to [-1, 1]\n",
    "def normalize_column(col):\n",
    "    return 2 * (col - col.min()) / (col.max() - col.min()) - 1\n",
    "\n",
    "df_numeric = df_numeric.apply(normalize_column, axis=0)\n",
    "\n",
    "def generate_matrix(rows, cols):\n",
    "    \"\"\"\n",
    "    Generate a matrix of shape (rows, cols) with values in (-1, 1).\n",
    "    Each column is drawn from either a truncated normal distribution \n",
    "    or a bimodal (two truncated normals combined), with 50% probability for each.\n",
    "\n",
    "    :param rows: number of rows\n",
    "    :param cols: number of columns\n",
    "    :return: A matrix of shape (rows, cols) with float32 values in (-1, 1).\n",
    "    \"\"\"\n",
    "    matrix = np.empty((rows, cols), dtype=np.float32)\n",
    "    \n",
    "    for col in tqdm(range(cols)):\n",
    "        # Randomly choose between normal distribution (50% chance) or a bimodal distribution\n",
    "        if np.random.rand() < 0.5:  \n",
    "            # Single truncated normal distribution\n",
    "            mu = np.random.uniform(-0.7, 0.7)\n",
    "            sigma = np.random.uniform(0.1, 0.3)\n",
    "            a, b = (-1 - mu) / sigma, (1 - mu) / sigma\n",
    "            data = truncnorm.rvs(a, b, loc=mu, scale=sigma, size=rows)\n",
    "        else:\n",
    "            # Bimodal distribution\n",
    "            mu1 = np.random.uniform(-0.8, -0.2)\n",
    "            mu2 = np.random.uniform(0.2, 0.8)\n",
    "            sigma1 = np.random.uniform(0.05, 0.15)\n",
    "            sigma2 = np.random.uniform(0.05, 0.15)\n",
    "            w1 = np.random.uniform(0.3, 0.7)\n",
    "            size1 = int(rows * w1)\n",
    "            size1 = max(1, min(size1, rows - 1))\n",
    "            size2 = rows - size1\n",
    "            \n",
    "            # Generate truncated normal for the first mode\n",
    "            a1, b1 = (-1 - mu1) / sigma1, (1 - mu1) / sigma1\n",
    "            data1 = truncnorm.rvs(a1, b1, loc=mu1, scale=sigma1, size=size1)\n",
    "            \n",
    "            # Generate truncated normal for the second mode\n",
    "            a2, b2 = (-1 - mu2) / sigma2, (1 - mu2) / sigma2\n",
    "            data2 = truncnorm.rvs(a2, b2, loc=mu2, scale=sigma2, size=size2)\n",
    "            \n",
    "            data = np.concatenate([data1, data2])\n",
    "            np.random.shuffle(data)  # Shuffle to mix the two modes\n",
    "        \n",
    "        # Randomly flip the sign of the generated column\n",
    "        sign = 1 if np.random.rand() < 0.5 else -1\n",
    "        matrix[:, col] = data * sign\n",
    "    \n",
    "    # Clip the matrix values to stay within (-1, 1), avoiding floating-point overflow\n",
    "    matrix = np.clip(matrix, -1 + 1e-8, 1 - 1e-8)\n",
    "    return matrix\n",
    "\n",
    "# Generate the matrix\n",
    "data = generate_matrix(2683, 20000)\n",
    "\n",
    "# Define input and output dimensions\n",
    "input_size = data.shape[0]          # e.g., 2683 (number of features)\n",
    "output_size = df_numeric.shape[1]   # e.g., 1800 (number of columns in the CSV)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Create Dataset and DataLoader\n",
    "# ------------------------------\n",
    "# Note: 'data' has the shape (input_size, num_samples), \n",
    "# so each sample is effectively one column of 'data'.\n",
    "samples = data.T  # shape: (num_samples, input_size)\n",
    "\n",
    "# Use sklearn to split into train/test sets\n",
    "train_samples, test_samples = train_test_split(samples, test_size=0.2, random_state=42)\n",
    "\n",
    "class HerbDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple Dataset wrapper for our sample data.\n",
    "    Expects 'samples' with shape (num_samples, input_size).\n",
    "    \"\"\"\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "train_dataset = HerbDataset(train_samples)\n",
    "test_dataset = HerbDataset(test_samples)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Construct a tensor for df_herb_nes to be used in the custom loss function\n",
    "# This has shape (output_size, input_size) because we use the transpose\n",
    "df_herb_nes_tensor = torch.tensor(df_numeric.values.T, dtype=torch.float32)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "df_herb_nes_tensor = df_herb_nes_tensor.to(device)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Model Definition\n",
    "# ------------------------------\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected network with variable hidden sizes, dropout, \n",
    "    and a final softplus activation to ensure non-negative outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes=(2048, 1024, 512, 256), output_size=1800, dropout_p=0.3):\n",
    "        super(Net, self).__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        for h_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_size, h_size))\n",
    "            layers.append(nn.BatchNorm1d(h_size))\n",
    "            layers.append(nn.ReLU())  # Using ReLU as activation\n",
    "            layers.append(nn.Dropout(dropout_p))\n",
    "            in_size = h_size\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # Use softplus to ensure non-negative output\n",
    "        x = F.softplus(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Custom Loss Function\n",
    "# ------------------------------\n",
    "# Original text indicates we want \"the Spearman's correlation between weighted_data and inputs \n",
    "# to be as negative as possible\", but here it's implemented as an absolute product-based approach.\n",
    "def custom_loss(logits, inputs, df_tensor, k, std_weight=1):\n",
    "    \"\"\"\n",
    "    logits: (batch_size, output_size)\n",
    "    inputs: (batch_size, input_size)\n",
    "    df_tensor: (output_size, input_size)\n",
    "    k: The number of top logits to keep\n",
    "    std_weight: An unused parameter here; keep for potential extensions\n",
    "\n",
    "    1. Select the top-k values in each row of 'logits' (by value).\n",
    "    2. Construct a sparse version of 'logits' that only keeps these top-k entries.\n",
    "    3. Multiply the sparse logits with 'df_tensor' to get 'weighted_data'.\n",
    "    4. Combine 'weighted_data' with 'inputs' (simply add).\n",
    "    5. Compute loss = mean of ( |(weighted_data + inputs) * inputs| / sum(inputs^2) ).\n",
    "\n",
    "    The idea is to measure how the added terms interact with the original inputs.\n",
    "    \"\"\"\n",
    "    # Select top-k values per row\n",
    "    topk_values, topk_indices = torch.topk(logits, k=k, dim=1)\n",
    "    modified_logits = torch.zeros_like(logits)\n",
    "    modified_logits.scatter_(1, topk_indices, topk_values)\n",
    "    \n",
    "    # Multiply with df_tensor to get weighted_data\n",
    "    weighted_data = torch.matmul(modified_logits, df_tensor)  # shape: (batch_size, input_size)\n",
    "    total = weighted_data + inputs\n",
    "\n",
    "    epsilon = 1e-8  # Prevent division by zero\n",
    "\n",
    "    # Numerator: sum of absolute products of 'total' and 'inputs'\n",
    "    numerator = torch.sum(torch.abs(total * inputs), dim=1)\n",
    "    \n",
    "    # Denominator: sum of squares of inputs\n",
    "    denominator = torch.sum(inputs * inputs, dim=1)\n",
    "    \n",
    "    # Compute per-sample loss and average\n",
    "    loss_per_sample = numerator / (denominator + epsilon)\n",
    "    return loss_per_sample.mean()\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Training and Evaluation\n",
    "# ------------------------------\n",
    "max_epochs = 1000    # Maximum training epochs\n",
    "patience = 5         # Early stopping patience\n",
    "k = 50               # Modify 'k' if needed\n",
    "\n",
    "model = Net(input_size=input_size, output_size=output_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)  # Added weight decay\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "best_loss = float('inf')\n",
    "epochs_without_improve = 0\n",
    "train_losses = []\n",
    "epoch = 0\n",
    "\n",
    "while epoch < max_epochs:\n",
    "    epoch += 1\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch}'):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch)\n",
    "        loss = custom_loss(logits, batch, df_herb_nes_tensor, k)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"Iteration 1 (k={k}) | Epoch {epoch}: \"\n",
    "          f\"Avg Train Loss = {avg_loss:.4f} | \"\n",
    "          f\"LR = {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Check for significant improvement\n",
    "    if avg_loss < best_loss - 1e-4:\n",
    "        best_loss = avg_loss\n",
    "        epochs_without_improve = 0\n",
    "    else:\n",
    "        epochs_without_improve += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_without_improve >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Testing phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch)\n",
    "        loss = custom_loss(logits, batch, df_herb_nes_tensor, k)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Iteration 1 (k={k}) | Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Plot training curve and save the figure\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.axhline(y=avg_test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Train/Test Loss for k = {k}')\n",
    "plt.legend()\n",
    "plt.text(0.05, 0.95, \n",
    "         f\"Final Epoch: {epoch}\\nFinal Train Loss: {train_losses[-1]:.4f}\\nTest Loss: {avg_test_loss:.4f}\",\n",
    "         transform=plt.gca().transAxes, verticalalignment='top',\n",
    "         bbox=dict(facecolor='white', alpha=0.8, boxstyle='round'))\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'weighted_loss_plot_k_{k}.png')\n",
    "plt.close()\n",
    "\n",
    "# Save the model parameters\n",
    "torch.save(model.state_dict(), f'weighted_herb_model_re_{k}.pth')\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
